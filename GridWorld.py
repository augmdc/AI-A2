# -*- coding: utf-8 -*-
"""
COSC-4117EL: Assignment 2 Problem Domain

This code provides a basic and interactive grid world environment where a robot can navigate using the arrow keys.
The robot encounters walls that block movement, gold that gives positive rewards, and traps that give negative rewards. The game ends when the robot reaches its goal.
The robot's score reflects the rewards it collects and penalties it incurs.

"""

import pygame
import numpy as np
import random
import time
import sys

# Imports of custom classes
import MDP
import QLearning

# Constants for our display
GRID_SIZE = 10  # Easily change this value
CELL_SIZE = 60  # Adjust this based on your display preferences
SCREEN_WIDTH = GRID_SIZE * CELL_SIZE
SCREEN_HEIGHT = GRID_SIZE * CELL_SIZE
GOLD_REWARD = 10 
TRAP_PENALTY = -10 
GOAL_REWARD = 30
WALL_VALUE = -1000
ROBOT_COLOR = (0, 128, 255)
GOAL_COLOR = (0, 255, 0)
WALL_COLOR = (0, 0, 0)
EMPTY_COLOR = (255, 255, 255)
GOLD_COLOR = (255, 255, 0)  # Yellow
TRAP_COLOR = (255, 0, 0)   # Red

# Change random seed for different results
random.seed(65)

class GridWorld:
    def __init__(self, size=GRID_SIZE):
        self.size = size
        self.grid = np.zeros((size, size))
        # Randomly select start and goal positions
        self.start = (random.randint(0, size-1), random.randint(0, size-1))
        self.goal = (random.randint(0, size-1), random.randint(0, size-1))
        self.robot_pos = self.start
        self.score = 0
        self.generate_walls_traps_gold()

    def generate_walls_traps_gold(self):
        for i in range(self.size):
            for j in range(self.size):
                if (i, j) != self.start and (i, j) != self.goal:
                    rand_num = random.random()
                    if rand_num < 0.1:  # 10% chance for a wall
                        self.grid[i][j] = WALL_VALUE
                    elif rand_num < 0.2:  # 20% chance for gold
                        self.grid[i][j] = GOLD_REWARD
                    elif rand_num < 0.3:  # 30% chance for a trap
                        self.grid[i][j] = TRAP_PENALTY

    def move(self, direction):
        """Move the robot in a given direction."""
        x, y = self.robot_pos
        # Conditions check for boundaries and walls
        if direction == "up" and x > 0 and self.grid[x-1][y] != WALL_VALUE:
            x -= 1
        elif direction == "down" and x < self.size-1 and self.grid[x+1][y] != WALL_VALUE:
            x += 1
        elif direction == "left" and y > 0 and self.grid[x][y-1] != WALL_VALUE:
            y -= 1
        elif direction == "right" and y < self.size-1 and self.grid[x][y+1] != WALL_VALUE:
            y += 1
        reward = self.grid[x][y] - 1  # step penalty
        self.robot_pos = (x, y)
        self.grid[x][y] = 0  # Clear the cell after the robot moves
        self.score += reward
        return reward

    def display(self):
        """Print a text-based representation of the grid world (useful for debugging)."""
        for i in range(self.size):
            row = ''
            for j in range(self.size):
                if (i, j) == self.robot_pos:
                    row += 'R '
                elif self.grid[i][j] == WALL_VALUE:
                    row += '# '
                else:
                    row += '. '
            print(row)
            
    def move_based_on_final_policy(self, final_policy_grid):
        """
        Moves the robot based on its position using the final_policy grid generated by the value iteration algorithm
        """
        x, y = self.robot_pos
        current_position_direction = final_policy_grid[x, y]
        directions_map = {"←": "left", "↑": "up", "→": "right", "↓": "down"}
        direction = directions_map.get(current_position_direction)
        
        if direction == "up" and x > 0 and self.grid[x-1][y] != WALL_VALUE:
            x -= 1
        elif direction == "down" and x < self.size-1 and self.grid[x+1][y] != WALL_VALUE:
            x += 1
        elif direction == "left" and y > 0 and self.grid[x][y-1] != WALL_VALUE:
            y -= 1
        elif direction == "right" and y < self.size-1 and self.grid[x][y+1] != WALL_VALUE:
            y += 1
        reward = self.grid[x][y] - 1  # step penalty
        self.robot_pos = (x, y)
        self.grid[x][y] = 0  # Clear the cell after the robot moves
        self.score += reward
        return reward
    
# Moves robot multiple times for an "episode"
def Q_learning(world, rewards, episodes, actions, gamma, epsilon, alpha, living_reward):
    q_values = np.zeros((10, 10, 4))
    
    # Every iteration is an episode
    # Decrease epsilon after every episode
    min_epsilon = 0.1
    decay_rate = 0.01

    for episode in range(episodes):
        steps = 0
        i, j = world.robot_pos # Capture initial position
        print(i, j)
        while steps < 10: 
            #Exploration
            if np.random.rand() < epsilon: 
                action_index = np.random.randint(0, len(actions))
           #Exploitation
            else:
                action_index = np.argmax(q_values[i, j])
                
            actual_action = actions[action_index]
            
            pi, pj = i, j # Past position
            i, j = world.robot_pos # Current position
            
            # Ensure the agent stays within the grid
            i = max(0, min(10 - 1, i))
            j = max(0, min(10- 1, j))
                
            # Q-learning update rule:
            # 1. Calculate the sample using the reward and the maximum Q-value of the next state
            sample = rewards[pi , pj] + gamma * np.max(q_values[i, j]) + living_reward

            # 2. Update Q-value using the Q-learning update rule (weighted average)
            q_values[pi, pj, action_index] = (1 - alpha) * q_values[pi, pj, action_index] + alpha * sample
            
            # If hits goal, restart episode
            if rewards[i, j] == GOAL_REWARD:
                break
            
            # If tried to move into a wall (boundary or actual), restart the episode
            if pi == i or pj == j:
                break
            
            print(f"Episode {episode}, step {steps}")
            print(q_values)
            steps += 1
            
        epsilon -= decay_rate
            
    return q_values
     
def setup_pygame():
    pygame.init()
    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
    pygame.display.set_caption("Grid World")
    clock = pygame.time.Clock()
    return screen, clock

def draw_grid(world, screen):
    """Render the grid, robot, and goal on the screen."""
    for i in range(GRID_SIZE):
        for j in range(GRID_SIZE):
            # Determine cell color based on its value
            color = EMPTY_COLOR
            cell_value = world.grid[i][j]
            if cell_value == WALL_VALUE:     # Wall
                color = WALL_COLOR           
            elif cell_value == GOLD_REWARD:  # Gold
                color = GOLD_COLOR
            elif cell_value == TRAP_PENALTY: # Trap
                color = TRAP_COLOR
            pygame.draw.rect(screen, color, pygame.Rect(j * CELL_SIZE, i * CELL_SIZE, CELL_SIZE, CELL_SIZE))

    # Drawing the grid lines
    for i in range(GRID_SIZE):
        pygame.draw.line(screen, (200, 200, 200), (i * CELL_SIZE, 0), (i * CELL_SIZE, SCREEN_HEIGHT))
        pygame.draw.line(screen, (200, 200, 200), (0, i * CELL_SIZE), (SCREEN_WIDTH, i * CELL_SIZE))

    pygame.draw.circle(screen, ROBOT_COLOR, 
                       (int((world.robot_pos[1] + 0.5) * CELL_SIZE), int((world.robot_pos[0] + 0.5) * CELL_SIZE)), 
                       int(CELL_SIZE/3))

    pygame.draw.circle(screen, GOAL_COLOR, 
                       (int((world.goal[1] + 0.5) * CELL_SIZE), int((world.goal[0] + 0.5) * CELL_SIZE)), 
                       int(CELL_SIZE/3))


def main():
    """Main loop"""
    screen, clock = setup_pygame()
    world = GridWorld()
    terminal_state = world.goal
    
    running = True
    
    """
    MDP Variable Values
    """
    
    # Set up State Space
    rewards = world.grid
    
    # Make goal large positive value in order to incentivize movement towards it
    rewards[terminal_state[0], terminal_state[1]] = GOAL_REWARD
    #print(rewards)
    
    n = world.size
    values = np.zeros((n, n))
    noise_prob = 0.2
    actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]
    
    final_values = QLearning.QLearning(rewards, actions, 50)
    #print(final_values)
    policy = QLearning.Qfinal_policy(n, n, rewards, final_values, actions)
    #print(policy)
    move_counter = 0  # Initialize a counter
    move_delay = 10  # Set the delay in game loops (adjust as needed)
    
    
    """
    Q-Learning Variable Values
    """
    #Q_learning(world, rewards, 1, actions, 0.9, 0.1, 0.5, -1)
    
    #sys.exit()
    
    algorithm = ""
    while algorithm != "0" and algorithm != "1":
        algorithm = input("Input either a 0 or 1. 0 - MDP, 1 - Q-Learning\n")
        print(algorithm)
    
    algorithm = int(algorithm)

    while running:
        # Rendering
        screen.fill(EMPTY_COLOR)
        draw_grid(world, screen)
        pygame.display.flip()
        clock.tick(5)  # FPS
        
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            #if event.type == pygame.KEYDOWN:
                #world.move_based_on_final_policy(policy)
        
        # MDP Section
        if algorithm == 0:
            if move_counter >= move_delay:
                world.move_based_on_final_policy(policy)
                move_counter = 0  # Reset the counter
                print(f"Current Score: {world.score}")
                # Check if the robot reached the goal
                if world.robot_pos == world.goal:
                    print("Robot reached the goal!")
                    print(f"Final Score: {world.score}")
                    running = False
                    break

            move_counter += 1  # Increment the counter
        else:
            
    pygame.quit()
    sys.exit()
    
if __name__ == "__main__":
    main()
